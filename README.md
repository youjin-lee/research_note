Research Note
=============

> This is for study related to Interpretable Machine Learning.
---------
### 읽은 논문

num | title | author | year | paper rink | note rink
---- | ---- | ---- | ---- | ---- | ----
1 | On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation | Sebastian Bach | PlosONE2015 | [LRP](https://pdfs.semanticscholar.org/17a2/73bbd4448083b01b5a9389b3c37f5425aac0.pdf?_ga=2.203325009.177220768.1611018563-57653733.1606442592) | [Note](https://drive.google.com/file/d/1YIakz1pZ69Zfcrd6ncU5SosyWE3uBTCR/view?usp=sharing)
2 | Learning Important Features Through Propagating Activation Differences| Avanti Shrikumar | ICML2017 | [DeepLIFT](https://arxiv.org/pdf/1704.02685.pdf) | [Note](https://drive.google.com/file/d/1FbCtEbHD-5mZ_3tZt8dZYdko6vD01bSr/view?usp=sharing)
3 | Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization | Ramprasaath R. Selvaraju | IJCV2019 | [Grad_CAM](https://arxiv.org/pdf/1610.02391.pdf) | [Note](https://drive.google.com/file/d/1zQHj8hRCtJAI7B6kxvACCFS_OOrKakCM/view?usp=sharing)
4 | "Why Should I Trust You?": Explaining the Predictions of Any Classifier | Marco Tulio Ribeiro | 2016 | [LIME](https://arxiv.org/pdf/1602.04938.pdf) | [Note](https://drive.google.com/file/d/1K_koIvaUdy2d2c1RMb2En5g3-N-hREzJ/view?usp=sharing)
5 | Examples are not Enough, Learn to Criticize!Criticism for Interpretability | Been Kim | NIPS2016 | [MMD](https://beenkim.github.io/papers/KIM2016NIPS_MMD.pdf) | [Note](https://drive.google.com/file/d/1guMTtrhLF8H6iLMVZjUJ9i_MC7Wn7Ugw/view?usp=sharing)
6 | Relative Attributing Propagation:Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks | Woo-Jeoung Nam | AAAI2020 | [RAP](https://arxiv.org/pdf/1904.00605.pdf) | [Note](https://drive.google.com/file/d/16BTSaOOZF1RgpSyLs6Cy--Flh8sE0qQM/view?usp=sharing)
7 | Learning how to explain neural networks: PatternNet and PatternAttribution | Pieter-Jan Kindermans | 2017 | [paper](https://arxiv.org/pdf/1705.05598.pdf) | [Note](https://drive.google.com/file/d/1AH48eO8hOZOoKYAd_cnxJDHHaWjmql9_/view?usp=sharing)


----------
### 읽을 논문 목록

num | title | author | year | citation | paper rink | note rink
---- | ---- | ---- | ---- | ---- | ---- | ----
1 | Towards better understanding of gradient-based attribution methods for Deep Neural Networks | Marco Ancona | ICLR 2018 | 327 | [paper](https://openreview.net/forum?id=Sy21R9JAW) | -
2 | Real Time Image Saliency for Black Box Classifiers | Piotr Dabkowski | NIPS 2017 | 214 | [paper](https://proceedings.neurips.cc/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html) | -
3 | Input Switched Affine Networks: An RNN Architecture Designed for Interpretability | Jakob N. Foerster | ICML 2017 | 24 | [paper](http://proceedings.mlr.press/v70/foerster17a.html) | -
4 | A Unified Approach to Interpreting Model Predictions | Scott M. Lundberg | NIPS 2017 | 2382 | [paper](https://arxiv.org/abs/1705.07874) | -
5 | Axiomatic Attribution for Deep Networks | Mukund Sundararajan | PMLR(proceeding ICML) 2017 | 1120 | [paper](http://proceedings.mlr.press/v70/sundararajan17a.html) | -
6 | Understanding Black-box Predictions via Influence Functions | Pang Wei Koh | ICML 2017 | 964 | [paper](http://proceedings.mlr.press/v70/koh17a.html) | -
7 | Learning to Segment Object Candidates | Pedro O. O. Pinheiro | NIPS 2015 | 644 | [paper](https://scholar.google.com/citations?user=BU6f7L4AAAAJ&hl=en) | -
8 | Network Dissection: Quantifying Interpretability of Deep Visual Representations | David Bau | CVPR 2017 | 663 | [paper](https://arxiv.org/abs/1704.05796) | -
9 | GAN Dissection: Visualizing and Understanding Generative Adversarial Networks | David Bau | ICLR 2019 | 166 | [paper](https://arxiv.org/abs/1811.10597) | -
10 | Visualizing and Understanding Recurrent Networks | Andrej Karpathy | ICML 2015 | 901 | [paper](https://arxiv.org/abs/1506.02078) | -
11 | Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps | Karen Simonyan | ICLR 2014 | 2362 | [paper](https://arxiv.org/abs/1312.6034) | -
12 | Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) | Been Kim | ICML 2018 | 337 | [paper](https://arxiv.org/abs/1711.11279) | -
13 | Sanity checks for saliency maps | Julius Adebayo | NIPS 2018 | 420 | [paper](https://arxiv.org/abs/1810.03292) | -
14 | Uncertainty-Aware Attention for Reliable Interpretation and Prediction | Jay Heo | NIPS 2018 | 35 | [paper](https://arxiv.org/abs/1805.09653) | -
15 | Interpretable basis decomposition for visual explanation | Bolei Zhou | ECCV 2018 | 82 | [paper](https://openaccess.thecvf.com/content_ECCV_2018/html/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.html) | -
16 | Towards Automatic Concept-based Explanations | Amirata Ghorbani | NIPS 2019 | 63 | [paper](https://arxiv.org/abs/1902.03129) | -
17 | Visualizing and Measuring the Geometry of BERT | Andy Coenen | NIPS 2019 | 55 | [paper](https://arxiv.org/abs/1906.02715) | -
18 | Poincaré Embeddings for Learning Hierarchical Representations | Maximillian Nickel  | NIPS 2017 | 467 | [paper](https://arxiv.org/abs/1705.08039) | -
19 | Visualizing and understanding convolutional networks | Matthew D Zeiler | ECCV 2014 | 11340 | [paper](https://arxiv.org/abs/1311.2901) | -
20 | Cost-Effective Interactive Attention Learning with Neural Attention Processes | Jay Heo | ICML 2020 | - | [paper](https://arxiv.org/abs/2006.05419) | -
21 | Neural Machine Translation by Jointly Learning to Align and Translate | Dzmitry Bahdanau | ICLR 2015 | 16450 | [paper](https://arxiv.org/abs/1409.0473) | -
22 | Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning | Yarin Gal | ICML 2016 | 2949 | [paper](https://arxiv.org/abs/1506.02142) | -
23 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention | Kelvin Xu | ICML 2015 | 6502 | [paper](https://arxiv.org/abs/1502.03044) | -
24 | An Efficient Explorative Sampling Considering the Generative Boundaries of Deep Generative Neural Networks | Giyoung Jeon | AAAI 2020 | 2 | [paper](https://ojs.aaai.org//index.php/AAAI/article/view/5852) | -
25 | Generating Visual Explanations | Lisa Anne Hendricks | ECCV 2016 | 364 | [paper](https://arxiv.org/abs/1603.08507) | -
26 | Visualizing and Understanding Atari Agents | Samuel Greydanus | PMLR(proceeding ICML) 2018 | 116 | [paper](http://proceedings.mlr.press/v80/greydanus18a.html) | -
27 | Neural Additive Models: Interpretable Machine Learning with Neural Nets | Rishabh Agarwal | 2020 | 17 | [paper](https://arxiv.org/abs/2004.13912) | -
28 | Explaining NonLinear Classification Decisions with Deep Taylor Decomposition | Grégoire Montavon | 2017 | 572 | [paper](https://www.sciencedirect.com/science/article/pii/S0031320316303582) | -



//index | paper_name | author | conference year | citation | [paper](address) | -

* https://xai.kaist.ac.kr/Tutorial/2020/#

