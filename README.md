Research Note
=============

> This is for study related to Interpretable Machine Learning.
---------
### 읽은 논문

num | title | author | year | paper rink | note rink
---- | ---- | ---- | ---- | ---- | ----
1 | On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation | Sebastian Bach | PlosONE2015 | [LRP](https://pdfs.semanticscholar.org/17a2/73bbd4448083b01b5a9389b3c37f5425aac0.pdf?_ga=2.203325009.177220768.1611018563-57653733.1606442592) | [Note](https://drive.google.com/file/d/1YIakz1pZ69Zfcrd6ncU5SosyWE3uBTCR/view?usp=sharing)
2 | Learning Important Features Through Propagating Activation Differences| Avanti Shrikumar | ICML2017 | [DeepLIFT](https://arxiv.org/pdf/1704.02685.pdf) | [Note](https://drive.google.com/file/d/1FbCtEbHD-5mZ_3tZt8dZYdko6vD01bSr/view?usp=sharing)
3 | Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization | Ramprasaath R. Selvaraju | IJCV2019 | [Grad_CAM](https://arxiv.org/pdf/1610.02391.pdf) | [Note](https://drive.google.com/file/d/1zQHj8hRCtJAI7B6kxvACCFS_OOrKakCM/view?usp=sharing)
4 | "Why Should I Trust You?": Explaining the Predictions of Any Classifier | Marco Tulio Ribeiro | 2016 | [LIME](https://arxiv.org/pdf/1602.04938.pdf) | [Note](https://drive.google.com/file/d/1K_koIvaUdy2d2c1RMb2En5g3-N-hREzJ/view?usp=sharing)
5 | Examples are not Enough, Learn to Criticize!Criticism for Interpretability | Been Kim | NIPS2016 | [MMD](https://beenkim.github.io/papers/KIM2016NIPS_MMD.pdf) | [Note](https://drive.google.com/file/d/1guMTtrhLF8H6iLMVZjUJ9i_MC7Wn7Ugw/view?usp=sharing)
6 | Relative Attributing Propagation:Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks | Woo-Jeoung Nam | AAAI2020 | [RAP](https://arxiv.org/pdf/1904.00605.pdf) | [Note](https://drive.google.com/file/d/16BTSaOOZF1RgpSyLs6Cy--Flh8sE0qQM/view?usp=sharing)
7 | Learning how to explain neural networks: PatternNet and PatternAttribution | Pieter-Jan Kindermans | 2017 | [paper](https://arxiv.org/pdf/1705.05598.pdf) | [Note](https://drive.google.com/file/d/1AH48eO8hOZOoKYAd_cnxJDHHaWjmql9_/view?usp=sharing)


----------
### 읽을 논문 목록

num | title | author | year | paper rink | note rink
---- | ---- | ---- | ---- | ---- | ----
1 | Explaining NonLinear Classification Decisions with Deep Taylor Decomposition | Gregoire Montavon | 2017 | [paper](https://arxiv.org/pdf/1512.02479.pdf) | -
2 | iNNvestigate Neural Networks! | Maximilian Alber | JMLR 2019 | [paper](https://jmlr.org/papers/v20/18-540.html) | -
3 | Towards better understanding of gradient-based attribution methods for Deep Neural Networks | Marco Ancona | ICLR 2018 | [paper](https://openreview.net/forum?id=Sy21R9JAW) | -
4 | How to Explain Individual Classification Decisions | David Baehrens | JMLR 2010 | [paper](https://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf) | -
5 | Real Time Image Saliency for Black Box Classifiers | Piotr Dabkowski | NIPS 2017 | [paper](https://proceedings.neurips.cc/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html) | -
6 | Input Switched Affine Networks: An RNN Architecture Designed for Interpretability | Jakob N. Foerster | ICMR 2017 | [paper](http://proceedings.mlr.press/v70/foerster17a.html) | -
7 | A Unified Approach to Interpreting Model Predictions | Scott M. Lundberg | NIPS 2017 | [paper](https://arxiv.org/abs/1705.07874) | -
8 | Axiomatic Attribution for Deep Networks | Mukund Sundararajan | ICML 2017 | [paper](http://proceedings.mlr.press/v70/sundararajan17a.html) | -
9 | Interpreting Deep Visual Representations via Network Dissection | Bolei Zhou | IEEE-TPAMI 2019 | [paper](https://arxiv.org/abs/1711.05611) | -
10 | iNNvestigat | Maximilian | JMLR 2019 | [paper](https) | -
